% Encoding: UTF-8

@Article{whygaussianity,
  author   = {Kiseon Kim and Georgy Shevlyakov},
  title    = {Why Gaussianity?},
  year     = {2008},
  volume   = {25},
  number   = {2},
  month    = {March},
  pages    = {102-113},
  issn     = {1053-5888},
  doi      = {10.1109/MSP.2007.913700},
  abstract = {In this article, we try to answer the question: "Why the ubiquitous use and success of the Gaussian distribution law?". The history of the Gaussian or normal distribution is rather long, having existed for nearly 300 years since it was discovered by de Moivre in 1733, and the related literature is immense. An extended and thorough treatment of the topic and a survey of the works in the related area are given in the posthumously edited book of E.T. Jaynes (2003), and we partially follow this source, in particular while considering the history of the posed question. The important aspects of the general history of noise, especially of Brownian motion, are given by Cohen (2005). Our main contribution to the topic is concerned with highlighting the role of Gaussian models in signal processing based on the optimal property of the Gaussian distribution minimizing Fisher information over the class of distributions with a bounded variance. We deal only with the univariate Gaussian distribution, omitting the properties of multivariate Gaussian distribution. First of all, we present the ideas of classical derivations of the Gaussian law. Then we consider its properties and characterizations including the central limit theorem (CLT) and minimization of the distribution entropy and Fisher information. Finally, we dwell on the connections between Gaussianity and robustness in signal processing.},
  journal  = {IEEE Signal Processing Magazine},
  keywords = {Gaussian distribution;entropy;signal processing;Brownian motion;Fisher information minimization;Gaussian distribution law;Gaussian models;central limit theorem;distribution entropy minimization;normal distribution;signal processing;univariate Gaussian distribution;Books;Entropy;Equations;Gaussian distribution;Gaussian processes;History;Limiting;Linear regression;Signal processing;Vectors},
}

@Article{deadd,
  author   = {P. Chatterjee and P. Milanfar},
  title    = {Is Denoising Dead?},
  year     = {2010},
  volume   = {19},
  number   = {4},
  month    = {April},
  pages    = {895-911},
  issn     = {1057-7149},
  doi      = {10.1109/TIP.2009.2037087},
  abstract = {Image denoising has been a well studied problem in the field of image processing. Yet researchers continue to focus attention on it to better the current state-of-the-art. Recently proposed methods take different approaches to the problem and yet their denoising performances are comparable. A pertinent question then to ask is whether there is a theoretical limit to denoising performance and, more importantly, are we there yet? As camera manufacturers continue to pack increasing numbers of pixels per unit area, an increase in noise sensitivity manifests itself in the form of a noisier image. We study the performance bounds for the image denoising problem. Our work in this paper estimates a lower bound on the mean squared error of the denoised result and compares the performance of current state-of-the-art denoising methods with this bound. We show that despite the phenomenal recent progress in the quality of denoising algorithms, some room for improvement still remains for a wide class of general images, and at certain signal-to-noise levels. Therefore, image denoising is not dead - yet.},
  journal  = {IEEE Transactions on Image Processing},
  keywords = {image denoising;camera manufacturers;denoising performance;image denoising;image processing;lower bound;mean squared error;noise sensitivity;Bayesian Cramér–Rao lower bound (CRLB);bias;bootstrapping;image denoising;mean squared error},
}

@Misc{gonzalez2002digital,
  author    = {Gonzalez, Rafael C and Woods, Richard E and others},
  title     = {Digital image processing},
  year      = {2002},
  publisher = {Prentice hall Upper Saddle River, NJ:},
}

@Comment{jabref-meta: databaseType:biblatex;}
